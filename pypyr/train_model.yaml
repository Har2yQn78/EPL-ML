vars:
  dataset_path: "dataset/EPL-2015-2025.csv"
  model_dir: "model"
  pipeline_filename: "full_prediction_pipeline.joblib"
  imputer_filename: "imputer.joblib"
  rolling_window_size: 7
  train_split_ratio: 0.82
  random_state: 42
  n_random_search_iter: 75
steps:
  - description: Load and prepare data
    run: run_python
    args:
      python: |
        import pandas as pd
        import os
        from sklearn.impute import SimpleImputer
        import numpy as np

        dataset_path = '{dataset_path}' # Get variable from pypyr context
        model_dir = '{model_dir}'
        imputer_filename = '{imputer_filename}'
        rolling_window_size = {rolling_window_size}

        # Ensure model directory exists
        os.makedirs(model_dir, exist_ok=True)
        imputer_path = os.path.join(model_dir, imputer_filename)

        # Load data
        df = pd.read_csv(dataset_path)

        # Convert MatchDate and sort
        df['MatchDate'] = pd.to_datetime(df['MatchDate'])
        df = df.sort_values(by='MatchDate').reset_index(drop=True)

        # --- Define rolling stats function (copied from notebook) ---
        def get_rolling_stats_refined(team, match_date, df_sorted, window_size, location_filter=None):
            team_matches = df_sorted[df_sorted['MatchDate'] < match_date].copy()
            team_matches_filtered = team_matches[(team_matches['HomeTeam'] == team) | (team_matches['AwayTeam'] == team)]

            if location_filter == 'Home':
                team_matches_filtered = team_matches_filtered[team_matches_filtered['HomeTeam'] == team]
            elif location_filter == 'Away':
                team_matches_filtered = team_matches_filtered[team_matches_filtered['AwayTeam'] == team]\

            recent_matches = team_matches_filtered.sort_values(by='MatchDate').tail(window_size)

            if recent_matches.empty:
                # Return NaNs for all expected stats if no prior matches
                stats = {'Points': np.nan, 'GoalsScored': np.nan, 'GoalsConceded': np.nan}
                for col in ['Shots', 'ShotsOnTarget', 'Corners', 'Fouls', 'YellowCards', 'RedCards']:
                     stats[col] = np.nan
                return stats

            stats = {}
            valid_matches_results = recent_matches.dropna(subset=['FullTimeResult', 'HomeTeam', 'AwayTeam'])

            if not valid_matches_results.empty:
                 points = valid_matches_results.apply(lambda row:\
                     3 if (row['HomeTeam'] == team and row['FullTimeResult'] == 'H') or \
                          (row['AwayTeam'] == team and row['FullTimeResult'] == 'A') else \
                     1 if row['FullTimeResult'] == 'D' else \
                     0, axis=1)
                 stats['Points'] = points.mean()
            else:
                 stats['Points'] = np.nan

            valid_matches_goals = recent_matches.dropna(subset=['FullTimeHomeGoals', 'FullTimeAwayGoals'])
            if not valid_matches_goals.empty:
                stats['GoalsScored'] = valid_matches_goals.apply(lambda row: row['FullTimeHomeGoals'] if row['HomeTeam'] == team else row['FullTimeAwayGoals'], axis=1).mean()
                stats['GoalsConceded'] = valid_matches_goals.apply(lambda row: row['FullTimeAwayGoals'] if row['HomeTeam'] == team else row['FullTimeHomeGoals'], axis=1).mean()
            else:
                stats['GoalsScored'] = np.nan
                stats['GoalsConceded'] = np.nan

            for col in ['Shots', 'ShotsOnTarget', 'Corners', 'Fouls', 'YellowCards', 'RedCards']:
                home_col = f'Home{col}'
                away_col = f'Away{col}'
                if home_col in recent_matches.columns and away_col in recent_matches.columns:
                     valid_matches_stat = recent_matches.dropna(subset=[home_col, away_col])
                     if not valid_matches_stat.empty:
                        stats[col] = valid_matches_stat.apply(lambda row: row[home_col] if row['HomeTeam'] == team else row[away_col], axis=1).mean()
                     else:
                        stats[col] = np.nan
                else:
                     stats[col] = np.nan # Handle case where columns might be missing

            return stats
        # --- End of rolling stats function ---


        # Calculate rolling stats for the entire dataset
        rolling_stats_list = []
        for index, row in df.iterrows():
            home_team = row['HomeTeam']
            away_team = row['AwayTeam']
            match_date = row['MatchDate']

            home_team_home_stats = get_rolling_stats_refined(home_team, match_date, df, rolling_window_size, location_filter='Home')
            home_team_overall_stats = get_rolling_stats_refined(home_team, match_date, df, rolling_window_size, location_filter=None)
            away_team_away_stats = get_rolling_stats_refined(away_team, match_date, df, rolling_window_size, location_filter='Away')
            away_team_overall_stats = get_rolling_stats_refined(away_team, match_date, df, rolling_window_size, location_filter=None)

            match_stats = {
                f'HomeAvgLast{rolling_window_size}_GoalsScored_Home': home_team_home_stats.get('GoalsScored', np.nan),
                f'HomeAvgLast{rolling_window_size}_GoalsConceded_Home': home_team_home_stats.get('GoalsConceded', np.nan),
                f'HomeAvgLast{rolling_window_size}_ShotsOnTarget_Home': home_team_home_stats.get('ShotsOnTarget', np.nan),
                f'HomeAvgLast{rolling_window_size}_Corners_Home': home_team_home_stats.get('Corners', np.nan),
                f'HomeAvgLast{rolling_window_size}_YellowCards_Home': home_team_home_stats.get('YellowCards', np.nan),
                f'HomeAvgLast{rolling_window_size}_Points_Home': home_team_home_stats.get('Points', np.nan),

                f'AwayAvgLast{rolling_window_size}_GoalsScored_Away': away_team_away_stats.get('GoalsScored', np.nan),
                f'AwayAvgLast{rolling_window_size}_GoalsConceded_Away': away_team_away_stats.get('GoalsConceded', np.nan),
                f'AwayAvgLast{rolling_window_size}_ShotsOnTarget_Away': away_team_away_stats.get('ShotsOnTarget', np.nan),
                f'AwayAvgLast{rolling_window_size}_Corners_Away': away_team_away_stats.get('Corners', np.nan),
                f'AwayAvgLast{rolling_window_size}_YellowCards_Away': away_team_away_stats.get('YellowCards', np.nan),
                f'AwayAvgLast{rolling_window_size}_Points_Away': away_team_away_stats.get('Points', np.nan),

                f'HomeAvgLast{rolling_window_size}_GoalsScored_Overall': home_team_overall_stats.get('GoalsScored', np.nan),
                f'HomeAvgLast{rolling_window_size}_GoalsConceded_Overall': home_team_overall_stats.get('GoalsConceded', np.nan),
                f'HomeAvgLast{rolling_window_size}_ShotsOnTarget_Overall': home_team_overall_stats.get('ShotsOnTarget', np.nan),
                f'HomeAvgLast{rolling_window_size}_Points_Overall': home_team_overall_stats.get('Points', np.nan),

                f'AwayAvgLast{rolling_window_size}_GoalsScored_Overall': away_team_overall_stats.get('GoalsScored', np.nan),
                f'AwayAvgLast{rolling_window_size}_GoalsConceded_Overall': away_team_overall_stats.get('GoalsConceded', np.nan),
                f'AwayAvgLast{rolling_window_size}_ShotsOnTarget_Overall': away_team_overall_stats.get('ShotsOnTarget', np.nan),
                f'AwayAvgLast{rolling_window_size}_Points_Overall': away_team_overall_stats.get('Points', np.nan),

                # Combined attack vs defense features
                f'HomeAttack_vs_AwayDefense_{rolling_window_size}Avg': home_team_home_stats.get('GoalsScored', np.nan) - away_team_away_stats.get('GoalsConceded', np.nan),
                f'AwayAttack_vs_HomeDefense_{rolling_window_size}Avg': away_team_away_stats.get('GoalsScored', np.nan) - home_team_home_stats.get('GoalsConceded', np.nan),
            }
            rolling_stats_list.append(match_stats)

        rolling_stats_df = pd.DataFrame(rolling_stats_list)
        df = pd.merge(df, rolling_stats_df, left_index=True, right_index=True, how='left')

        # Define features and target
        TARGET = 'FullTimeResult'
        y = df[TARGET].copy()
        rolling_avg_cols = [col for col in df.columns if f'AvgLast{rolling_window_size}' in col and 'Attack_vs_Defense' not in col] # Exclude combined features initially, add back later if needed

        # Rebuild features list explicitly based on notebook Cell 5
        features = [
          f'HomeAvgLast{rolling_window_size}_GoalsScored_Home',
          f'HomeAvgLast{rolling_window_size}_GoalsConceded_Home',
          f'HomeAvgLast{rolling_window_size}_ShotsOnTarget_Home',
          f'HomeAvgLast{rolling_window_size}_Corners_Home',
          f'HomeAvgLast{rolling_window_size}_YellowCards_Home',
          f'HomeAvgLast{rolling_window_size}_Points_Home',
          f'AwayAvgLast{rolling_window_size}_GoalsScored_Away',
          f'AwayAvgLast{rolling_window_size}_GoalsConceded_Away',
          f'AwayAvgLast{rolling_window_size}_ShotsOnTarget_Away',
          f'AwayAvgLast{rolling_window_size}_Corners_Away',
          f'AwayAvgLast{rolling_window_size}_YellowCards_Away',
          f'AwayAvgLast{rolling_window_size}_Points_Away',
          f'HomeAvgLast{rolling_window_size}_GoalsScored_Overall',
          f'HomeAvgLast{rolling_window_size}_GoalsConceded_Overall',
          f'HomeAvgLast{rolling_window_size}_ShotsOnTarget_Overall',
          f'HomeAvgLast{rolling_window_size}_Points_Overall',
          f'AwayAvgLast{rolling_window_size}_GoalsScored_Overall',
          f'AwayAvgLast{rolling_window_size}_GoalsConceded_Overall',
          f'AwayAvgLast{rolling_window_size}_ShotsOnTarget_Overall',
          f'AwayAvgLast{rolling_window_size}_Points_Overall'
        ]

        # Check if the combined features should be added as in Cell 4 output, re-evaluating Cell 4 output.
        # Looking at Cell 4 output and Cell 5 feature list, only the _Overall and _Home/_Away rolling averages are used.
        # The Attack_vs_Defense features seem to be calculated but NOT explicitly added to the feature list 'features' in Cell 5.
        # Let's stick to the feature list defined in Cell 5 for consistency with the saved model.
        X = df[features].copy()

        # Impute missing values
        imputer = SimpleImputer(missing_values=np.nan, strategy='median')
        imputer.fit(X)
        X_imputed_array = imputer.transform(X)
        X_imputed = pd.DataFrame(X_imputed_array, columns=X.columns, index=X.index)
        X = X_imputed # Update X to the imputed DataFrame

        # Store processed data and imputer in pypyr context
        # Note: Cannot pass full dataframes directly, pass their paths or derived objects
        # For this step, we just process and pass the relevant parts for the next steps.
        # The full original df_sorted is needed by the prediction function,
        # so we'll need to load it separately in the FastAPI app.
        # Here we focus on getting X_train, y_train, and the imputer ready.

        # Save imputer for prediction
        import joblib
        joblib.dump(imputer, imputer_path)
        print(f"Imputer saved to {imputer_path}")

        # Store the necessary data for the next steps in the context
        # Store as byte strings or paths is complex. Let's assume data is small enough or
        # use a simpler approach for pypyr: just pass X_train, y_train derived indexes/shapes
        # or re-load derived data if paths are saved.
        # Simpler: Calculate X_train, y_train and pass their index/shape, and reference the loaded X, y via paths.
        # But run_python is good for keeping logic together. Let's store objects directly if possible (pypyr constraints)
        # Pypyr context handles basic types, not large objects like dataframes.
        # The most practical way is to split data and pass X_train, y_train, X_test, y_test indexes/shapes
        # and re-create them in the next step OR pass paths to temporary files.
        # OR, calculate X_train, y_train and pass as dictionaries/lists (inefficient for large data).
        # OR, keep training logic in one large run_python step. Let's go with one step for simplicity in this example.

        # Redo this step to do all data prep and split
        # Keep imputer saving here
        print("Data loading and preparation complete.")

  - description: Split data and train model
    run: run_python
    args:
      python: |
        import pandas as pd
        import numpy as np
        import joblib
        import os
        from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold
        from sklearn.preprocessing import StandardScaler
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.pipeline import Pipeline
        from sklearn.impute import SimpleImputer
        from scipy.stats import randint
        import warnings

        warnings.filterwarnings('ignore')

        # Get variables from pypyr context
        dataset_path = '{dataset_path}'
        model_dir = '{model_dir}'
        pipeline_filename = '{pipeline_filename}'
        imputer_filename = '{imputer_filename}'
        rolling_window_size = {rolling_window_size}
        train_split_ratio = {train_split_ratio}
        random_state = {random_state}
        n_random_search_iter = {n_random_search_iter}

        pipeline_path = os.path.join(model_dir, pipeline_filename)
        imputer_path = os.path.join(model_dir, imputer_filename)

        # Re-load data and do processing up to imputation (this is redundant, better to do in one step)
        # Let's combine data prep/split/train into one run_python for simplicity in pypyr context handling
        # --- REVISING: Let's make this step load the *processed* data or re-run the processing ---
        # Re-loading and reprocessing is simpler for pypyr than passing dataframes.
        # This means the data prep logic is duplicated slightly, but the pipeline steps are clear.

        # --- Redo Data Prep (same as first step's core logic) ---
        df = pd.read_csv(dataset_path)
        df['MatchDate'] = pd.to_datetime(df['MatchDate'])
        df = df.sort_values(by='MatchDate').reset_index(drop=True)

        # --- Define rolling stats function (copied again for self-contained step) ---
        def get_rolling_stats_refined(team, match_date, df_sorted, window_size, location_filter=None):
            team_matches = df_sorted[df_sorted['MatchDate'] < match_date].copy()
            team_matches_filtered = team_matches[(team_matches['HomeTeam'] == team) | (team_matches['AwayTeam'] == team)]
            if location_filter == 'Home': team_matches_filtered = team_matches_filtered[team_matches_filtered['HomeTeam'] == team]
            elif location_filter == 'Away': team_matches_filtered = team_matches_filtered[team_matches_filtered['AwayTeam'] == team]
            recent_matches = team_matches_filtered.sort_values(by='MatchDate').tail(window_size)
            if recent_matches.empty:
                stats = {'Points': np.nan, 'GoalsScored': np.nan, 'GoalsConceded': np.nan}
                for col in ['Shots', 'ShotsOnTarget', 'Corners', 'Fouls', 'YellowCards', 'RedCards']: stats[col] = np.nan
                return stats
            stats = {}
            valid_matches_results = recent_matches.dropna(subset=['FullTimeResult', 'HomeTeam', 'AwayTeam'])
            if not valid_matches_results.empty:
                 points = valid_matches_results.apply(lambda row:\
                     3 if (row['HomeTeam'] == team and row['FullTimeResult'] == 'H') or \
                          (row['AwayTeam'] == team and row['FullTimeResult'] == 'A') else \
                     1 if row['FullTimeResult'] == 'D' else \
                     0, axis=1)
                 stats['Points'] = points.mean()
            else: stats['Points'] = np.nan
            valid_matches_goals = recent_matches.dropna(subset=['FullTimeHomeGoals', 'FullTimeAwayGoals'])
            if not valid_matches_goals.empty:
                stats['GoalsScored'] = valid_matches_goals.apply(lambda row: row['FullTimeHomeGoals'] if row['HomeTeam'] == team else row['FullTimeAwayGoals'], axis=1).mean()
                stats['GoalsConceded'] = valid_matches_goals.apply(lambda row: row['FullTimeAwayGoals'] if row['HomeTeam'] == team else row['FullTimeHomeGoals'], axis=1).mean()
            else:
                stats['GoalsScored'] = np.nan
                stats['GoalsConceded'] = np.nan
            for col in ['Shots', 'ShotsOnTarget', 'Corners', 'Fouls', 'YellowCards', 'RedCards']:
                home_col = f'Home{col}'
                away_col = f'Away{col}'
                if home_col in recent_matches.columns and away_col in recent_matches.columns:
                     valid_matches_stat = recent_matches.dropna(subset=[home_col, away_col])
                     if not valid_matches_stat.empty:
                        stats[col] = valid_matches_stat.apply(lambda row: row[home_col] if row['HomeTeam'] == team else row[away_col], axis=1).mean()
                     else: stats[col] = np.nan
                else: stats[col] = np.nan
            return stats
        # --- End of rolling stats function ---

        # Calculate rolling stats for the entire dataset
        rolling_stats_list = []
        # Using tqdm here might be tricky in pypyr's run_python, omit for simplicity
        # from tqdm.notebook import tqdm # If you must keep it, but requires notebook env
        # iterator = df.iterrows()
        # if 'tqdm' in globals() and 'tqdm_notebook' in globals(): iterator = tqdm(df.iterrows(), total=df.shape[0], desc=f"Calculating {rolling_window_size}-Match Rolling Stats")

        for index, row in df.iterrows():
            home_team = row['HomeTeam']
            away_team = row['AwayTeam']
            match_date = row['MatchDate']
            home_team_home_stats = get_rolling_stats_refined(home_team, match_date, df, rolling_window_size, location_filter='Home')
            home_team_overall_stats = get_rolling_stats_refined(home_team, match_date, df, rolling_window_size, location_filter=None)
            away_team_away_stats = get_rolling_stats_refined(away_team, match_date, df, rolling_window_size, location_filter='Away')
            away_team_overall_stats = get_rolling_stats_refined(away_team, match_date, df, rolling_window_size, location_filter=None)
            match_stats = {
                f'HomeAvgLast{rolling_window_size}_GoalsScored_Home': home_team_home_stats.get('GoalsScored', np.nan),
                f'HomeAvgLast{rolling_window_size}_GoalsConceded_Home': home_team_home_stats.get('GoalsConceded', np.nan),
                f'HomeAvgLast{rolling_window_size}_ShotsOnTarget_Home': home_team_home_stats.get('ShotsOnTarget', np.nan),
                f'HomeAvgLast{rolling_window_size}_Corners_Home': home_team_home_stats.get('Corners', np.nan),
                f'HomeAvgLast{rolling_window_size}_YellowCards_Home': home_team_home_stats.get('YellowCards', np.nan),
                f'HomeAvgLast{rolling_window_size}_Points_Home': home_team_home_stats.get('Points', np.nan),
                f'AwayAvgLast{rolling_window_size}_GoalsScored_Away': away_team_away_stats.get('GoalsScored', np.nan),
                f'AwayAvgLast{rolling_window_size}_GoalsConceded_Away': away_team_away_stats.get('GoalsConceded', np.nan),
                f'AwayAvgLast{rolling_window_size}_ShotsOnTarget_Away': away_team_away_stats.get('ShotsOnTarget', np.nan),
                f'AwayAvgLast{rolling_window_size}_Corners_Away': away_team_away_stats.get('Corners', np.nan),
                f'AwayAvgLast{rolling_window_size}_YellowCards_Away': away_team_away_stats.get('YellowCards', np.nan),
                f'AwayAvgLast{rolling_window_size}_Points_Away': away_team_away_stats.get('Points', np.nan),
                f'HomeAvgLast{rolling_window_size}_GoalsScored_Overall': home_team_overall_stats.get('GoalsScored', np.nan),
                f'HomeAvgLast{rolling_window_size}_GoalsConceded_Overall': home_team_overall_stats.get('GoalsConceded', np.nan),
                f'HomeAvgLast{rolling_window_size}_ShotsOnTarget_Overall': home_team_overall_stats.get('ShotsOnTarget', np.nan),
                f'HomeAvgLast{rolling_window_size}_Points_Overall': home_team_overall_stats.get('Points', np.nan),
                f'AwayAvgLast{rolling_window_size}_GoalsScored_Overall': away_team_overall_stats.get('GoalsScored', np.nan),
                f'AwayAvgLast{rolling_window_size}_GoalsConceded_Overall': away_team_overall_stats.get('GoalsConceded', np.nan),
                f'AwayAvgLast{rolling_window_size}_ShotsOnTarget_Overall': away_team_overall_stats.get('ShotsOnTarget', np.nan),
                f'AwayAvgLast{rolling_window_size}_Points_Overall': away_team_overall_stats.get('Points', np.nan)
            }
            rolling_stats_list.append(match_stats)

        rolling_stats_df = pd.DataFrame(rolling_stats_list)
        df = pd.merge(df, rolling_stats_df, left_index=True, right_index=True, how='left')

        TARGET = 'FullTimeResult'
        y = df[TARGET].copy()
        features = [
          f'HomeAvgLast{rolling_window_size}_GoalsScored_Home',
          f'HomeAvgLast{rolling_window_size}_GoalsConceded_Home',
          f'HomeAvgLast{rolling_window_size}_ShotsOnTarget_Home',
          f'HomeAvgLast{rolling_window_size}_Corners_Home',
          f'HomeAvgLast{rolling_window_size}_YellowCards_Home',
          f'HomeAvgLast{rolling_window_size}_Points_Home',
          f'AwayAvgLast{rolling_window_size}_GoalsScored_Away',
          f'AwayAvgLast{rolling_window_size}_GoalsConceded_Away',
          f'AwayAvgLast{rolling_window_size}_ShotsOnTarget_Away',
          f'AwayAvgLast{rolling_window_size}_Corners_Away',
          f'AwayAvgLast{rolling_window_size}_YellowCards_Away',
          f'AwayAvgLast{rolling_window_size}_Points_Away',
          f'HomeAvgLast{rolling_window_size}_GoalsScored_Overall',
          f'HomeAvgLast{rolling_window_size}_GoalsConceded_Overall',
          f'HomeAvgLast{rolling_window_size}_ShotsOnTarget_Overall',
          f'HomeAvgLast{rolling_window_size}_Points_Overall',
          f'AwayAvgLast{rolling_window_size}_GoalsScored_Overall',
          f'AwayAvgLast{rolling_window_size}_GoalsConceded_Overall',
          f'AwayAvgLast{rolling_window_size}_ShotsOnTarget_Overall',
          f'AwayAvgLast{rolling_window_size}_Points_Overall'
        ]
        X = df[features].copy()

        # Impute missing values (fit on full X, transform full X)
        imputer = SimpleImputer(missing_values=np.nan, strategy='median')
        imputer.fit(X) # Fit imputer
        X_imputed_array = imputer.transform(X)
        X_imputed = pd.DataFrame(X_imputed_array, columns=X.columns, index=X.index)
        X = X_imputed # Update X

        # Save the fitted imputer
        joblib.dump(imputer, imputer_path)
        print(f"Imputer saved to {imputer_path}")

        # --- End Redo Data Prep ---

        # Split data chronologically
        split_index = int(len(X) * train_split_ratio)
        X_train, X_test = X[:split_index], X[split_index:]
        y_train, y_test = y[:split_index], y[split_index:]

        print(f"Data split into {train_split_ratio*100:.2f}% train and {(1-train_split_ratio)*100:.2f}% test chronologically.")
        print(f"Training data shape: {X_train.shape}")
        print(f"Testing data shape: {X_test.shape}")

        # Define model pipeline
        rf_model = RandomForestClassifier(random_state=random_state)
        model_pipeline = Pipeline([
            ('scaler', StandardScaler()),
            ('classifier', rf_model)
        ])

        # Hyperparameter tuning
        print("Starting Hyperparameter Tuning using RandomizedSearchCV...")
        param_distributions = {
            'classifier__n_estimators': randint(100, 500),
            'classifier__max_depth': randint(5, 25),
            'classifier__min_samples_split': randint(2, 20),
            'classifier__min_samples_leaf': randint(1, 15),
            'classifier__max_features': ['sqrt', 'log2', None],
            'classifier__class_weight': ['balanced', 'balanced_subsample', None]
        }
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)
        random_search = RandomizedSearchCV(
            estimator=model_pipeline,
            param_distributions=param_distributions,
            n_iter=n_random_search_iter,
            scoring='neg_log_loss',
            cv=cv,
            verbose=1, # Set verbose lower for cleaner pypyr output
            random_state=random_state,
            n_jobs=-1,
            return_train_score=False
        )

        # Fit the search
        random_search.fit(X_train, y_train)
        print("Hyperparameter tuning complete.")
        print("Best parameters found:")
        print(random_search.best_params_)
        print(f"Best cross-validation score (Negative Log Loss): {random_search.best_score_:.4f}")

        # Get the final model pipeline
        final_pipeline = random_search.best_estimator_

        # Save the final pipeline
        joblib.dump(final_pipeline, pipeline_path)
        print(f"Final model pipeline saved to: {pipeline_path}")

        print("Model training and saving complete.")